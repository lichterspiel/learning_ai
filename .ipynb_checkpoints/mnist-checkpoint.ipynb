{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "245ab565-8397-4b7b-8eb1-d51abd4ba5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max accuracy i achieved was 93% revisit or rewrite\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "18348420-88c6-4762-aa93-07e001269ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_to_np(image_num, path):\n",
    "    f = gzip.open(path, \"r\")\n",
    "\n",
    "    f.read(4)# magic number\n",
    "    num_images = int.from_bytes(f.read(4),byteorder='big')\n",
    "    num_rows = int.from_bytes(f.read(4),byteorder='big')\n",
    "    num_columns = int.from_bytes(f.read(4),byteorder='big')\n",
    "    \n",
    "    buf = f.read(num_rows * num_columns * image_num)\n",
    "    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "    data = data.reshape(-1, 28, 28)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "22b691f0-5dba-4675-88d5-37c24d6631ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_labels_to_np(label_num, path):\n",
    "    f = gzip.open(path, \"r\")\n",
    "\n",
    "    f.read(4)# magic number\n",
    "    num_images = int.from_bytes(f.read(4),byteorder='big')\n",
    "    \n",
    "    buf = f.read(num_images)\n",
    "    data = np.frombuffer(buf, dtype=np.uint8)\n",
    "    #data = data.reshape(-1,10)\n",
    "    f.close()\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dffd45aa-42f3-4570-a818-cea825df463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_training = \"../mnist/train-images-idx3-ubyte.gz\"\n",
    "path_labels = \"../mnist/train-labels-idx1-ubyte.gz\"\n",
    "data = load_mnist_to_np(60000,path_training)\n",
    "labels = load_labels_to_np(60000,path_labels)\n",
    "training = list(zip(data,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "046bccd8-4a4a-4ffa-b147-08377f01a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # f(x) = 1 / (1+e^-x)\n",
    "    return 1.0 / (1.0 + np.exp( -x ))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    #Derivative of the sigmoid function\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d3ef3225-94dd-4d4f-bf2c-3374041c7e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function (loss function ? is it the same)\n",
    "def mse_loss(y_true, y_pred):\n",
    "    # numpy arrays of same length\n",
    "    return ((y_true-y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e554e5d7-8239-46ec-aed0-b330dd5e63fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.uniform(0,0,(y, 1)) for y in sizes[1:]]\n",
    "        self.weights = [np.random.uniform(-0.5, 0.5,(y, x))\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        c = a.reshape(-1, 784).T\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            c = sigmoid(np.dot(w,c) + b)\n",
    "        return np.argmax(c)\n",
    "    \n",
    "    def SGD(self, training_data,labels, epochs,batch_size=128, learn_rate=0.01, test_data=None):\n",
    "        for j in range(epochs):\n",
    "            # gibt mir 128 bilder zur√ºck \n",
    "            #randint geht von 0 bis hier 3000 (3000 Bilder) und 128 zahlen also random bilder\n",
    "            # wenn in ein numpy array ein array index wird geht er jeden wert durch holt die matrix \n",
    "            # und speichert alle in einem neuen array\n",
    "            samp = np.random.randint(0, data.shape[0], size = batch_size)\n",
    "            X = training_data[samp].reshape(-1, 28*28)\n",
    "            X = X / 255\n",
    "            Y = labels[samp]\n",
    "        \n",
    "            #backprop\n",
    "            #self.backprop(training_data,labels, learn_rate)\n",
    "            self.backprop(X,Y, learn_rate)\n",
    "            \n",
    "            if test_data:\n",
    "                print (f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n",
    "            else:\n",
    "                pass\n",
    "                #print (f\"Epoch {j} complete\")    \n",
    "                \n",
    "    def backprop(self, X, Y, learn_rate):\n",
    "        nr_correct = 0\n",
    "        for i in range(len(X)):\n",
    "            activation = X[i].reshape(784,1)\n",
    "            # save the activations for backproping\n",
    "            activations = []\n",
    "            # input goes into the activation array \n",
    "            activations.append(activation)\n",
    "            # save the values before activation function\n",
    "            zl = []\n",
    "            # forward pass\n",
    "            for b, w in zip(self.biases, self.weights):\n",
    "                z = np.dot(w, activation) + b\n",
    "                zl.append(z)\n",
    "                \n",
    "                activation = sigmoid(z)\n",
    "                activations.append(activation)\n",
    "         \n",
    "            \n",
    "            # transform label from a number to a matrix which is zero except for that number\n",
    "            y = np.zeros((10,1), np.float32)\n",
    "            y[Y[i],range(y.shape[1])] = 1\n",
    "            \n",
    "            nr_correct += int(np.argmax(activations[-1]) == np.argmax(y))\n",
    "            #backwards pass\n",
    "            #substract output_layer - label \n",
    "            delta = (activations[-1] - y)\n",
    "            #delta = delta * sigmoid_deriv(zl[-1])\n",
    "            \n",
    "            # first gradient descent\n",
    "            # multiply the error times the output of the layer before and adjust it with the learning_rate\n",
    "            self.weights[-1] += -learn_rate * np.dot(delta, activations[-2].T)\n",
    "            self.biases[-1] += learn_rate * delta\n",
    "            \n",
    "            #backward pass loop start from first hidden layer\n",
    "            for l in range(2, self.num_layers):\n",
    "                # undo activation function of the layer\n",
    "                z = zl[-l]\n",
    "                da_z = sigmoid_deriv(z)\n",
    "                \n",
    "                # delta between guess and output aka error //dc_a * da_z\n",
    "                delta = np.dot(self.weights[-l + 1].T, delta) * da_z\n",
    "                \n",
    "                #multiply delta with the activation of previous layer to get the gradient\n",
    "                dc_w = np.dot(delta,activations[-l - 1].T)\n",
    "                \n",
    "                #gradient descent\n",
    "                self.weights[-l] += -learn_rate * dc_w\n",
    "                self.biases[-l] += -learn_rate * delta\n",
    "            \n",
    "        #print(nr_correct)\n",
    "        print(f\"Acc: {round((nr_correct / X.shape[0]) * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "95d461bb-02ad-4775-874a-3238090768a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 9.38%\n",
      "Acc: 8.59%\n",
      "Acc: 13.28%\n",
      "Acc: 11.72%\n",
      "Acc: 16.41%\n",
      "Acc: 21.09%\n",
      "Acc: 25.0%\n",
      "Acc: 24.22%\n",
      "Acc: 23.44%\n",
      "Acc: 19.53%\n",
      "Acc: 28.91%\n",
      "Acc: 26.56%\n",
      "Acc: 30.47%\n",
      "Acc: 35.16%\n",
      "Acc: 39.84%\n",
      "Acc: 32.81%\n",
      "Acc: 31.25%\n",
      "Acc: 42.19%\n",
      "Acc: 39.06%\n",
      "Acc: 40.62%\n",
      "Acc: 34.38%\n",
      "Acc: 37.5%\n",
      "Acc: 41.41%\n",
      "Acc: 46.09%\n",
      "Acc: 49.22%\n",
      "Acc: 45.31%\n",
      "Acc: 52.34%\n",
      "Acc: 52.34%\n",
      "Acc: 43.75%\n",
      "Acc: 52.34%\n",
      "Acc: 53.12%\n",
      "Acc: 49.22%\n",
      "Acc: 55.47%\n",
      "Acc: 53.91%\n",
      "Acc: 62.5%\n",
      "Acc: 57.81%\n",
      "Acc: 60.94%\n",
      "Acc: 61.72%\n",
      "Acc: 64.84%\n",
      "Acc: 53.12%\n",
      "Acc: 61.72%\n",
      "Acc: 53.91%\n",
      "Acc: 72.66%\n",
      "Acc: 58.59%\n",
      "Acc: 74.22%\n",
      "Acc: 57.81%\n",
      "Acc: 60.16%\n",
      "Acc: 64.06%\n",
      "Acc: 71.88%\n",
      "Acc: 58.59%\n",
      "Acc: 72.66%\n",
      "Acc: 57.81%\n",
      "Acc: 67.19%\n",
      "Acc: 60.94%\n",
      "Acc: 66.41%\n",
      "Acc: 66.41%\n",
      "Acc: 69.53%\n",
      "Acc: 72.66%\n",
      "Acc: 71.09%\n",
      "Acc: 70.31%\n",
      "Acc: 72.66%\n",
      "Acc: 68.75%\n",
      "Acc: 68.75%\n",
      "Acc: 66.41%\n",
      "Acc: 70.31%\n",
      "Acc: 64.06%\n",
      "Acc: 72.66%\n",
      "Acc: 78.12%\n",
      "Acc: 69.53%\n",
      "Acc: 74.22%\n",
      "Acc: 73.44%\n",
      "Acc: 75.0%\n",
      "Acc: 74.22%\n",
      "Acc: 77.34%\n",
      "Acc: 76.56%\n",
      "Acc: 67.97%\n",
      "Acc: 72.66%\n",
      "Acc: 75.78%\n",
      "Acc: 77.34%\n",
      "Acc: 74.22%\n",
      "Acc: 75.0%\n",
      "Acc: 75.0%\n",
      "Acc: 75.78%\n",
      "Acc: 68.75%\n",
      "Acc: 80.47%\n",
      "Acc: 78.91%\n",
      "Acc: 73.44%\n",
      "Acc: 76.56%\n",
      "Acc: 76.56%\n",
      "Acc: 78.12%\n",
      "Acc: 78.12%\n",
      "Acc: 71.88%\n",
      "Acc: 77.34%\n",
      "Acc: 75.78%\n",
      "Acc: 81.25%\n",
      "Acc: 74.22%\n",
      "Acc: 74.22%\n",
      "Acc: 78.91%\n",
      "Acc: 76.56%\n",
      "Acc: 74.22%\n",
      "Acc: 75.78%\n",
      "Acc: 78.12%\n",
      "Acc: 76.56%\n",
      "Acc: 80.47%\n",
      "Acc: 79.69%\n",
      "Acc: 77.34%\n",
      "Acc: 75.0%\n",
      "Acc: 78.91%\n",
      "Acc: 72.66%\n",
      "Acc: 75.78%\n",
      "Acc: 78.91%\n",
      "Acc: 79.69%\n",
      "Acc: 79.69%\n",
      "Acc: 85.94%\n",
      "Acc: 79.69%\n",
      "Acc: 75.0%\n",
      "Acc: 76.56%\n",
      "Acc: 80.47%\n",
      "Acc: 75.0%\n",
      "Acc: 88.28%\n",
      "Acc: 78.91%\n",
      "Acc: 78.12%\n",
      "Acc: 76.56%\n",
      "Acc: 82.81%\n",
      "Acc: 74.22%\n",
      "Acc: 84.38%\n",
      "Acc: 83.59%\n",
      "Acc: 83.59%\n",
      "Acc: 80.47%\n",
      "Acc: 78.12%\n",
      "Acc: 77.34%\n",
      "Acc: 80.47%\n",
      "Acc: 86.72%\n",
      "Acc: 78.91%\n",
      "Acc: 81.25%\n",
      "Acc: 80.47%\n",
      "Acc: 81.25%\n",
      "Acc: 77.34%\n",
      "Acc: 76.56%\n",
      "Acc: 78.12%\n",
      "Acc: 85.16%\n",
      "Acc: 78.91%\n",
      "Acc: 79.69%\n",
      "Acc: 79.69%\n",
      "Acc: 78.91%\n",
      "Acc: 77.34%\n",
      "Acc: 71.09%\n",
      "Acc: 78.91%\n",
      "Acc: 82.81%\n",
      "Acc: 82.81%\n",
      "Acc: 81.25%\n",
      "Acc: 81.25%\n",
      "Acc: 85.16%\n",
      "Acc: 85.94%\n",
      "Acc: 82.03%\n",
      "Acc: 82.81%\n",
      "Acc: 81.25%\n",
      "Acc: 82.03%\n",
      "Acc: 82.03%\n",
      "Acc: 82.81%\n",
      "Acc: 71.88%\n",
      "Acc: 71.88%\n",
      "Acc: 82.03%\n",
      "Acc: 83.59%\n",
      "Acc: 82.81%\n",
      "Acc: 83.59%\n",
      "Acc: 85.16%\n",
      "Acc: 80.47%\n",
      "Acc: 75.0%\n",
      "Acc: 79.69%\n",
      "Acc: 78.91%\n",
      "Acc: 81.25%\n",
      "Acc: 81.25%\n",
      "Acc: 76.56%\n",
      "Acc: 74.22%\n",
      "Acc: 79.69%\n",
      "Acc: 87.5%\n",
      "Acc: 81.25%\n",
      "Acc: 83.59%\n",
      "Acc: 76.56%\n",
      "Acc: 85.16%\n",
      "Acc: 73.44%\n",
      "Acc: 82.03%\n",
      "Acc: 82.81%\n",
      "Acc: 82.03%\n",
      "Acc: 78.91%\n",
      "Acc: 78.12%\n",
      "Acc: 81.25%\n",
      "Acc: 75.78%\n",
      "Acc: 83.59%\n",
      "Acc: 82.81%\n",
      "Acc: 85.16%\n",
      "Acc: 79.69%\n",
      "Acc: 83.59%\n",
      "Acc: 82.03%\n",
      "Acc: 86.72%\n",
      "Acc: 84.38%\n",
      "Acc: 78.12%\n",
      "Acc: 86.72%\n",
      "Acc: 75.78%\n",
      "Acc: 78.12%\n",
      "Acc: 88.28%\n",
      "Acc: 85.94%\n",
      "Acc: 81.25%\n",
      "Acc: 81.25%\n",
      "Acc: 83.59%\n",
      "Acc: 82.81%\n",
      "Acc: 81.25%\n",
      "Acc: 82.03%\n",
      "Acc: 82.03%\n",
      "Acc: 83.59%\n",
      "Acc: 85.94%\n",
      "Acc: 81.25%\n",
      "Acc: 78.91%\n",
      "Acc: 79.69%\n",
      "Acc: 82.03%\n",
      "Acc: 82.03%\n",
      "Acc: 84.38%\n",
      "Acc: 79.69%\n",
      "Acc: 78.12%\n",
      "Acc: 75.78%\n",
      "Acc: 85.16%\n",
      "Acc: 84.38%\n",
      "Acc: 85.16%\n",
      "Acc: 89.06%\n",
      "Acc: 84.38%\n",
      "Acc: 84.38%\n",
      "Acc: 86.72%\n",
      "Acc: 83.59%\n",
      "Acc: 82.81%\n",
      "Acc: 85.94%\n",
      "Acc: 87.5%\n",
      "Acc: 88.28%\n",
      "Acc: 89.06%\n",
      "Acc: 82.81%\n",
      "Acc: 85.94%\n",
      "Acc: 89.84%\n",
      "Acc: 83.59%\n",
      "Acc: 85.16%\n",
      "Acc: 79.69%\n",
      "Acc: 85.94%\n",
      "Acc: 84.38%\n",
      "Acc: 86.72%\n",
      "Acc: 82.03%\n",
      "Acc: 85.94%\n",
      "Acc: 86.72%\n",
      "Acc: 84.38%\n",
      "Acc: 84.38%\n",
      "Acc: 86.72%\n",
      "Acc: 86.72%\n",
      "Acc: 83.59%\n",
      "Acc: 88.28%\n",
      "Acc: 83.59%\n",
      "Acc: 89.06%\n",
      "Acc: 85.16%\n",
      "Acc: 85.16%\n",
      "Acc: 91.41%\n",
      "Acc: 82.81%\n",
      "Acc: 87.5%\n",
      "Acc: 88.28%\n",
      "Acc: 85.16%\n",
      "Acc: 86.72%\n",
      "Acc: 81.25%\n",
      "Acc: 86.72%\n",
      "Acc: 88.28%\n",
      "Acc: 87.5%\n",
      "Acc: 83.59%\n",
      "Acc: 82.03%\n",
      "Acc: 78.91%\n",
      "Acc: 86.72%\n",
      "Acc: 75.0%\n",
      "Acc: 87.5%\n",
      "Acc: 85.94%\n",
      "Acc: 85.94%\n",
      "Acc: 81.25%\n",
      "Acc: 88.28%\n",
      "Acc: 89.06%\n",
      "Acc: 84.38%\n",
      "Acc: 82.03%\n",
      "Acc: 80.47%\n",
      "Acc: 82.03%\n",
      "Acc: 83.59%\n",
      "Acc: 84.38%\n",
      "Acc: 85.94%\n",
      "Acc: 82.03%\n",
      "Acc: 85.16%\n",
      "Acc: 80.47%\n",
      "Acc: 83.59%\n",
      "Acc: 78.91%\n",
      "Acc: 91.41%\n",
      "Acc: 85.94%\n",
      "Acc: 87.5%\n",
      "Acc: 85.16%\n",
      "Acc: 89.06%\n",
      "Acc: 85.94%\n",
      "Acc: 88.28%\n",
      "Acc: 84.38%\n",
      "Acc: 82.03%\n",
      "Acc: 88.28%\n",
      "Acc: 84.38%\n",
      "Acc: 83.59%\n",
      "Acc: 83.59%\n",
      "Acc: 85.16%\n",
      "Acc: 85.16%\n",
      "Acc: 79.69%\n",
      "Acc: 94.53%\n",
      "Acc: 91.41%\n",
      "Acc: 85.94%\n",
      "Acc: 82.03%\n",
      "Acc: 87.5%\n",
      "Acc: 85.94%\n",
      "Acc: 79.69%\n",
      "Acc: 86.72%\n",
      "Acc: 83.59%\n",
      "Acc: 83.59%\n",
      "Acc: 85.16%\n",
      "Acc: 92.97%\n",
      "Acc: 86.72%\n",
      "Acc: 88.28%\n",
      "Acc: 92.19%\n",
      "Acc: 86.72%\n",
      "Acc: 85.94%\n",
      "Acc: 90.62%\n",
      "Acc: 85.94%\n",
      "Acc: 81.25%\n",
      "Acc: 89.06%\n",
      "Acc: 81.25%\n",
      "Acc: 85.94%\n",
      "Acc: 85.94%\n",
      "Acc: 88.28%\n",
      "Acc: 89.84%\n",
      "Acc: 89.06%\n",
      "Acc: 87.5%\n",
      "Acc: 89.06%\n",
      "Acc: 96.09%\n",
      "Acc: 85.16%\n",
      "Acc: 84.38%\n",
      "Acc: 81.25%\n",
      "Acc: 84.38%\n",
      "Acc: 83.59%\n",
      "Acc: 88.28%\n",
      "Acc: 81.25%\n",
      "Acc: 86.72%\n",
      "Acc: 86.72%\n",
      "Acc: 86.72%\n",
      "Acc: 84.38%\n",
      "Acc: 88.28%\n",
      "Acc: 88.28%\n",
      "Acc: 82.81%\n",
      "Acc: 85.94%\n",
      "Acc: 86.72%\n",
      "Acc: 86.72%\n",
      "Acc: 85.16%\n",
      "Acc: 82.03%\n",
      "Acc: 85.16%\n",
      "Acc: 83.59%\n",
      "Acc: 88.28%\n",
      "Acc: 89.06%\n",
      "Acc: 81.25%\n",
      "Acc: 88.28%\n",
      "Acc: 89.84%\n",
      "Acc: 86.72%\n",
      "Acc: 88.28%\n",
      "Acc: 87.5%\n",
      "Acc: 88.28%\n",
      "Acc: 89.06%\n",
      "Acc: 89.84%\n",
      "Acc: 85.94%\n",
      "Acc: 89.84%\n",
      "Acc: 86.72%\n",
      "Acc: 86.72%\n",
      "Acc: 89.06%\n",
      "Acc: 72.66%\n",
      "Acc: 90.62%\n",
      "Acc: 85.16%\n",
      "Acc: 88.28%\n",
      "Acc: 80.47%\n",
      "Acc: 82.81%\n",
      "Acc: 86.72%\n",
      "Acc: 85.94%\n",
      "Acc: 84.38%\n",
      "Acc: 91.41%\n",
      "Acc: 90.62%\n",
      "Acc: 85.94%\n",
      "Acc: 85.94%\n",
      "Acc: 89.84%\n",
      "Acc: 82.03%\n",
      "Acc: 85.94%\n",
      "Acc: 85.94%\n",
      "Acc: 82.81%\n",
      "Acc: 84.38%\n",
      "Acc: 89.06%\n",
      "Acc: 88.28%\n",
      "Acc: 83.59%\n",
      "Acc: 86.72%\n",
      "Acc: 85.94%\n",
      "Acc: 87.5%\n",
      "Acc: 84.38%\n",
      "Acc: 86.72%\n",
      "Acc: 85.94%\n",
      "Acc: 85.94%\n",
      "Acc: 85.16%\n",
      "Acc: 85.94%\n",
      "Acc: 82.03%\n",
      "Acc: 84.38%\n",
      "Acc: 88.28%\n",
      "Acc: 85.16%\n",
      "Acc: 79.69%\n",
      "Acc: 88.28%\n",
      "Acc: 88.28%\n",
      "Acc: 88.28%\n",
      "Acc: 85.16%\n",
      "Acc: 82.81%\n",
      "Acc: 86.72%\n",
      "Acc: 88.28%\n",
      "Acc: 89.84%\n",
      "Acc: 89.84%\n",
      "Acc: 82.81%\n",
      "Acc: 89.06%\n",
      "Acc: 84.38%\n",
      "Acc: 86.72%\n",
      "Acc: 89.06%\n",
      "Acc: 83.59%\n",
      "Acc: 88.28%\n",
      "Acc: 92.19%\n",
      "Acc: 83.59%\n",
      "Acc: 86.72%\n",
      "Acc: 87.5%\n",
      "Acc: 87.5%\n",
      "Acc: 81.25%\n",
      "Acc: 86.72%\n",
      "Acc: 85.16%\n",
      "Acc: 86.72%\n",
      "Acc: 82.81%\n",
      "Acc: 89.84%\n",
      "Acc: 92.19%\n",
      "Acc: 89.84%\n",
      "Acc: 89.84%\n",
      "Acc: 91.41%\n",
      "Acc: 87.5%\n",
      "Acc: 85.16%\n",
      "Acc: 85.94%\n",
      "Acc: 85.94%\n",
      "Acc: 86.72%\n",
      "Acc: 85.94%\n",
      "Acc: 87.5%\n",
      "Acc: 91.41%\n",
      "Acc: 85.94%\n",
      "Acc: 89.06%\n",
      "Acc: 89.06%\n",
      "Acc: 88.28%\n",
      "Acc: 85.94%\n",
      "Acc: 82.81%\n",
      "Acc: 87.5%\n",
      "Acc: 87.5%\n",
      "Acc: 89.06%\n",
      "Acc: 85.16%\n",
      "Acc: 87.5%\n",
      "Acc: 84.38%\n",
      "Acc: 92.19%\n",
      "Acc: 86.72%\n",
      "Acc: 86.72%\n",
      "Acc: 90.62%\n",
      "Acc: 89.84%\n",
      "Acc: 83.59%\n",
      "Acc: 85.16%\n",
      "Acc: 85.94%\n",
      "Acc: 87.5%\n",
      "Acc: 90.62%\n",
      "Acc: 92.97%\n",
      "Acc: 89.06%\n",
      "Acc: 91.41%\n",
      "Acc: 86.72%\n",
      "Acc: 84.38%\n",
      "Acc: 85.94%\n",
      "Acc: 83.59%\n",
      "Acc: 86.72%\n",
      "Acc: 88.28%\n",
      "Acc: 82.03%\n",
      "Acc: 91.41%\n",
      "Acc: 91.41%\n",
      "Acc: 86.72%\n",
      "Acc: 90.62%\n",
      "Acc: 87.5%\n",
      "Acc: 89.84%\n",
      "Acc: 89.84%\n",
      "Acc: 85.94%\n",
      "Acc: 90.62%\n",
      "Acc: 89.06%\n",
      "Acc: 85.94%\n",
      "Acc: 87.5%\n",
      "Acc: 92.97%\n",
      "Acc: 87.5%\n",
      "Acc: 88.28%\n",
      "Acc: 85.16%\n",
      "Acc: 89.84%\n",
      "Acc: 89.06%\n",
      "Acc: 86.72%\n",
      "Acc: 89.84%\n",
      "Acc: 93.75%\n"
     ]
    }
   ],
   "source": [
    "n = Network([784,128,10])\n",
    "\n",
    "n.SGD(data,labels,500,128,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a98d113f-ac70-4d82-bea1-7e8341e1329b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAORElEQVR4nO3df6zV9X3H8dfLywUUIYPiECmKLcSWri2uN7KmxrjZGmVppGlmyhpiNxf8o2jbdJnWxdTsR2OXtqZLOxMsTNZYbROrsk47kTZzTSx6tVRQ7LAWJxRBRx3aDLjAe3/cr+aq93zu5Zzv+XF5Px/JyTnn+z7f+3nn6Ivv95zv+X4/jggBOPGd1O0GAHQGYQeSIOxAEoQdSIKwA0lM6uRgkz0lpmpaJ4cEUjmo3+pwHPJotZbCbvsSSV+X1CfpWxFxU+n1UzVNS31RK0MCKNgcmxrWmt6Nt90n6ZuSLpW0WNIK24ub/XsA2quVz+znSXomIp6NiMOS7pR0WT1tAahbK2GfJ+n5Ec93VcvewPYq24O2B4d0qIXhALSi7d/GR8SaiBiIiIF+TWn3cAAaaCXsuyXNH/H87dUyAD2olbA/KmmR7bNtT5b0CUkb6mkLQN2aPvQWEUdsr5b07xo+9LYuIp6srTMAtWrpOHtE3Cfpvpp6AdBG/FwWSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQ6OmUzRte38Oxi/cP3/LxYv3rmjjrbeYPF315drC+88WfF+rGDB+tsBy1gyw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSXCcvQP6zllYrG//3Kxi/e6ZvyjWjx13R+P31MpvFOu/WlE+jv43u/+4Ye2X33hXcd0Z3/lpsY7j01LYbe+U9Iqko5KORMRAHU0BqF8dW/Y/jIiXavg7ANqIz+xAEq2GPSQ9YPsx26tGe4HtVbYHbQ8O6VCLwwFoVqu78edHxG7bvytpo+2nI+KhkS+IiDWS1kjSDM+KFscD0KSWtuwRsbu63yfpbknn1dEUgPo1HXbb02xPf+2xpIslbaurMQD1amU3fo6ku22/9ne+ExE/rKWrE8yB976tWH/6o+Vj2b3srEmTi/W1Z21sWHvxph8U173+6mXF+v98svz7hCPP7izWs2k67BHxrKT319gLgDbi0BuQBGEHkiDsQBKEHUiCsANJcIprB5z8Qvlnwn+28+Ji/Z8XPFBnOz3jtL4pxfqtZ24q1q+584Ji/fk/bXyJ7qPP/Kq47omILTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJOGIzl08ZoZnxVJf1LHxJoq+mTOL9R1fKF9yec4jjS8mvf9dfcV1h2aU//u//4Pl6aDXnv2vxfpUd++nHO9bd03D2oIbHu5gJ52zOTbpQOz3aDW27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBMfZ0ZL/W16eF2TXR0Y95CtJ2r68vZfQvuXlRQ1r97/nd9o6drdwnB0AYQeyIOxAEoQdSIKwA0kQdiAJwg4kwXXj0ZKT73mkWF/44pKGtc2X9hfXXTplqJmWXvf4gTML1QMt/e2JaMwtu+11tvfZ3jZi2SzbG23vqO7LV18A0HXj2Y2/TdIlb1p2naRNEbFI0qbqOYAeNmbYI+IhSfvftPgySeurx+slLa+3LQB1a/Yz+5yI2FM9fkHSnEYvtL1K0ipJmqpTmhwOQKta/jY+hs+kaXg2TUSsiYiBiBjoV3kiPwDt02zY99qeK0nV/b76WgLQDs2GfYOkK6rHV0i6t552ALTLmJ/Zbd8h6UJJs23vkvRFSTdJ+p7tKyU9J+nydjaZ3aR5ZxTrccrUDnVy/I797YsNa60eR//fY4eL9We+vrhhbbp+2tLYE9GYYY+IFQ1KXIUCmED4uSyQBGEHkiDsQBKEHUiCsANJcIprD/Ck8n+Gg+vL9fve/d0623mDk8bYHhxT4+mi2+3v9l5YrE+/M9/htRK27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBMfZe8DhP1pSrN//7ls608gE82//+YFifWHC01hL2LIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBIcZ++AvvecU6x/8h/vKdbHOqe8nfrdV6wPNZwLqHWLb19drC/8q4fbN/gJiC07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBcfYOOHZyf7E+o+9gef0uXpt9rOPo7ext/gPlKZlxfMbcstteZ3uf7W0jlt1oe7ftLdVtWXvbBNCq8ezG3ybpklGW3xwRS6rbffW2BaBuY4Y9Ih6StL8DvQBoo1a+oFtt+4lqN39moxfZXmV70PbgkA61MByAVjQb9lskvVPSEkl7JH210QsjYk1EDETEQL+mNDkcgFY1FfaI2BsRRyPimKRbJZ1Xb1sA6tZU2G3PHfH0Y5K2NXotgN4w5nF223dIulDSbNu7JH1R0oW2l0gKSTslXdW+Fie+GCz/W/jNv/iTYv0Lq4/U2c5x+fiiLcX6Dac93raxD/3lb4r1/gfbNvQJacywR8SKURavbUMvANqIn8sCSRB2IAnCDiRB2IEkCDuQhCPaeC3gN5nhWbHUF3VsPLRu0rwzivVFG/YV618+vfnLPf/6SPnn1X9+1eeK9ck/fLTpsSeqzbFJB2K/R6uxZQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJLiUdAcMffgDxfq5X/5Zsb7t5fKx7oM3N65P/cEjxXXHcmT3r4v1HR8/s1gf/FHjKZ8HphwtrnvGpPKVjYamlbdVk4vVfNiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASHGevwaTT5xTrh699qVj/0umbywOcXi7v/6fG531ffe3y4rpbH1pUrM/7j6FivVyVrtqysmHtsaW3jbF22SsrDxTr0+5q6c+fcNiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASHGevwdNfKZ9v/tTiNW0df1Zf4/O+b3/H/eWVx6if9Kny9uCYjpX/fhvNPvW3XRt7Ihpzy257vu0f237K9pO2P1Mtn2V7o+0d1f3M9rcLoFnj2Y0/IunzEbFY0h9I+rTtxZKuk7QpIhZJ2lQ9B9Cjxgx7ROyJiMerx69I2i5pnqTLJK2vXrZe0vI29QigBsf1md32AknnStosaU5E7KlKL0ga9QfitldJWiVJU3VK040CaM24v423faqkuyR9NiLecAZCDM8OOeoMkRGxJiIGImKgX+ULCAJon3GF3Xa/hoN+e0R8v1q81/bcqj5XUnk6TwBdNeZuvG1LWitpe0R8bURpg6QrJN1U3d/blg4ngHP+/tVi/UvvXVKsXz97S33NJHLolrnF+iT9d4c6mRjG85n9Q5JWStpqe0u17HoNh/x7tq+U9Jyky9vSIYBajBn2iPiJpFEnd5d0Ub3tAGgXfi4LJEHYgSQIO5AEYQeSIOxAEpziWoOj23cU6z+68fxi/fKbB1saf05f49NMp580cScuXvPywmJ9xtbyJbrLE0Lnw5YdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Lw8EVmOmOGZ8VSc6Jc3V5e+cGGtd8sLq87cMHTxfr6BQ8W6+28lPT71l1TrC+44eG2jT1RbY5NOhD7Rz1LlS07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBcXbgBMJxdgCEHciCsANJEHYgCcIOJEHYgSQIO5DEmGG3Pd/2j20/ZftJ25+plt9oe7ftLdVtWfvbBdCs8UwScUTS5yPicdvTJT1me2NVuzkivtK+9gDUZTzzs++RtKd6/Irt7ZLmtbsxAPU6rs/sthdIOlfS5mrRattP2F5ne2aDdVbZHrQ9OKRDrXULoGnjDrvtUyXdJemzEXFA0i2S3ilpiYa3/F8dbb2IWBMRAxEx0K8prXcMoCnjCrvtfg0H/faI+L4kRcTeiDgaEcck3SrpvPa1CaBV4/k23pLWStoeEV8bsXzuiJd9TNK2+tsDUJfxfBv/IUkrJW21vaVadr2kFbaXSApJOyVd1Yb+ANRkPN/G/0TSaOfH3ld/OwDahV/QAUkQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkujolM22X5T03IhFsyW91LEGjk+v9tarfUn01qw6ezsrIk4brdDRsL9lcHswIga61kBBr/bWq31J9NasTvXGbjyQBGEHkuh22Nd0efySXu2tV/uS6K1ZHemtq5/ZAXROt7fsADqEsANJdCXsti+x/Qvbz9i+rhs9NGJ7p+2t1TTUg13uZZ3tfba3jVg2y/ZG2zuq+1Hn2OtSbz0xjXdhmvGuvnfdnv6845/ZbfdJ+i9JH5G0S9KjklZExFMdbaQB2zslDURE13+AYfsCSa9K+peI+L1q2T9I2h8RN1X/UM6MiGt7pLcbJb3a7Wm8q9mK5o6cZlzSckmfUhffu0Jfl6sD71s3tuznSXomIp6NiMOS7pR0WRf66HkR8ZCk/W9afJmk9dXj9Rr+n6XjGvTWEyJiT0Q8Xj1+RdJr04x39b0r9NUR3Qj7PEnPj3i+S70133tIesD2Y7ZXdbuZUcyJiD3V4xckzelmM6MYcxrvTnrTNOM98941M/15q/iC7q3Oj4jfl3SppE9Xu6s9KYY/g/XSsdNxTePdKaNMM/66br53zU5/3qpuhH23pPkjnr+9WtYTImJ3db9P0t3qvamo9742g251v6/L/byul6bxHm2acfXAe9fN6c+7EfZHJS2yfbbtyZI+IWlDF/p4C9vTqi9OZHuapIvVe1NRb5B0RfX4Ckn3drGXN+iVabwbTTOuLr93XZ/+PCI6fpO0TMPfyP9S0l93o4cGfb1D0s+r25Pd7k3SHRrerRvS8HcbV0p6m6RNknZIelDSrB7q7duStkp6QsPBmtul3s7X8C76E5K2VLdl3X7vCn115H3j57JAEnxBByRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ/D9nVzvwjYYxxQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xb/tql77jt13z7fd6cqtftw62nm0000gn/T/ipykernel_7703/2810796055.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp( -x ))\n"
     ]
    }
   ],
   "source": [
    "import random as r\n",
    "x = r.randrange(0,60000)\n",
    "image = np.asarray(data[x].squeeze())\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "print(n.feedforward(data[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed8a873-c268-4e40-a9b6-0f4a2f30f61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725c6fa-fd00-49d4-a4f8-79cf06596571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
