{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "245ab565-8397-4b7b-8eb1-d51abd4ba5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max accuracy i achieved was 93% revisit or rewrite\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "18348420-88c6-4762-aa93-07e001269ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_to_np(image_num, path):\n",
    "    f = gzip.open(path, \"r\")\n",
    "\n",
    "    f.read(4)# magic number\n",
    "    num_images = int.from_bytes(f.read(4),byteorder='big')\n",
    "    num_rows = int.from_bytes(f.read(4),byteorder='big')\n",
    "    num_columns = int.from_bytes(f.read(4),byteorder='big')\n",
    "    \n",
    "    buf = f.read(num_rows * num_columns * image_num)\n",
    "    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "    data = data.reshape(-1, 28, 28)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "22b691f0-5dba-4675-88d5-37c24d6631ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_labels_to_np(label_num, path):\n",
    "    f = gzip.open(path, \"r\")\n",
    "\n",
    "    f.read(4)# magic number\n",
    "    num_images = int.from_bytes(f.read(4),byteorder='big')\n",
    "    \n",
    "    buf = f.read(num_images)\n",
    "    data = np.frombuffer(buf, dtype=np.uint8)\n",
    "    #data = data.reshape(-1,10)\n",
    "    f.close()\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dffd45aa-42f3-4570-a818-cea825df463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_training = \"../mnist/train-images-idx3-ubyte.gz\"\n",
    "path_labels = \"../mnist/train-labels-idx1-ubyte.gz\"\n",
    "data = load_mnist_to_np(60000,path_training)\n",
    "labels = load_labels_to_np(60000,path_labels)\n",
    "training = list(zip(data,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "046bccd8-4a4a-4ffa-b147-08377f01a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # f(x) = 1 / (1+e^-x)\n",
    "    return 1.0 / (1.0 + np.exp( -x ))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    #Derivative of the sigmoid function\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d3ef3225-94dd-4d4f-bf2c-3374041c7e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function (loss function ? is it the same)\n",
    "def mse_loss(y_true, y_pred):\n",
    "    # numpy arrays of same length\n",
    "    return ((y_true-y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e554e5d7-8239-46ec-aed0-b330dd5e63fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.uniform(0,0,(y, 1)) for y in sizes[1:]]\n",
    "        self.weights = [np.random.uniform(-0.5, 0.5,(y, x))\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        c = a.reshape(-1, 784).T\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            c = sigmoid(np.dot(w,c) + b)\n",
    "        return np.argmax(c)\n",
    "    \n",
    "    def SGD(self, training_data,labels, epochs,batch_size=128, learn_rate=0.01, test_data=None):\n",
    "        for j in range(epochs):\n",
    "            # gibt mir 128 bilder zur√ºck \n",
    "            #randint geht von 0 bis hier 3000 (3000 Bilder) und 128 zahlen also random bilder\n",
    "            # wenn in ein numpy array ein array index wird geht er jeden wert durch holt die matrix \n",
    "            # und speichert alle in einem neuen array\n",
    "            samp = np.random.randint(0, data.shape[0], size = batch_size)\n",
    "            X = training_data[samp].reshape(-1, 28*28)\n",
    "            X = X / 255\n",
    "            Y = labels[samp]\n",
    "        \n",
    "            #backprop\n",
    "            #self.backprop(training_data,labels, learn_rate)\n",
    "            self.backprop(X,Y, learn_rate)\n",
    "            \n",
    "            if test_data:\n",
    "                print (f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n",
    "            else:\n",
    "                pass\n",
    "                #print (f\"Epoch {j} complete\")    \n",
    "                \n",
    "    def backprop(self, X, Y, learn_rate):\n",
    "        nr_correct = 0\n",
    "        for i in range(len(X)):\n",
    "            activation = X[i].reshape(784,1)\n",
    "            # save the activations for backproping\n",
    "            activations = []\n",
    "            # input goes into the activation array \n",
    "            activations.append(activation)\n",
    "            # save the values before activation function\n",
    "            zl = []\n",
    "            # forward pass\n",
    "            for b, w in zip(self.biases, self.weights):\n",
    "                z = np.dot(w, activation) + b\n",
    "                zl.append(z)\n",
    "                \n",
    "                activation = sigmoid(z)\n",
    "                activations.append(activation)\n",
    "         \n",
    "            \n",
    "            # transform label from a number to a matrix which is zero except for that number\n",
    "            y = np.zeros((10,1), np.float32)\n",
    "            y[Y[i],range(y.shape[1])] = 1\n",
    "            \n",
    "            nr_correct += int(np.argmax(activations[-1]) == np.argmax(y))\n",
    "            #backwards pass\n",
    "            #substract output_layer - label \n",
    "            delta = (activations[-1] - y)\n",
    "            #delta = delta * sigmoid_deriv(zl[-1])\n",
    "            \n",
    "            # first gradient descent\n",
    "            # multiply the error times the output of the layer before and adjust it with the learning_rate\n",
    "            self.weights[-1] += -learn_rate * np.dot(delta, activations[-2].T)\n",
    "            self.biases[-1] += learn_rate * delta\n",
    "            \n",
    "            #backward pass loop start from first hidden layer\n",
    "            for l in range(2, self.num_layers):\n",
    "                # undo activation function of the layer\n",
    "                z = zl[-l]\n",
    "                da_z = sigmoid_deriv(z)\n",
    "                \n",
    "                # delta between guess and output aka error //dc_a * da_z\n",
    "                delta = np.dot(self.weights[-l + 1].T, delta) * da_z\n",
    "                \n",
    "                #multiply delta with the activation of previous layer to get the gradient\n",
    "                dc_w = np.dot(delta,activations[-l - 1].T)\n",
    "                \n",
    "                #gradient descent\n",
    "                self.weights[-l] += -learn_rate * dc_w\n",
    "                self.biases[-l] += -learn_rate * delta\n",
    "            \n",
    "        #print(nr_correct)\n",
    "        print(f\"Acc: {round((nr_correct / X.shape[0]) * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "95d461bb-02ad-4775-874a-3238090768a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 9.38%\n",
      "Acc: 8.59%\n",
      "Acc: 13.28%\n",
      "Acc: 11.72%\n",
      "Acc: 16.41%\n",
      "Acc: 21.09%\n",
      "Acc: 25.0%\n",
      "Acc: 24.22%\n",
      "Acc: 23.44%\n",
      "Acc: 19.53%\n",
      "Acc: 28.91%\n",
      "Acc: 26.56%\n",
      "Acc: 30.47%\n",
      "Acc: 35.16%\n",
      "Acc: 39.84%\n",
      "Acc: 32.81%\n",
      "Acc: 31.25%\n",
      "Acc: 42.19%\n",
      "Acc: 39.06%\n",
      "Acc: 40.62%\n",
      "Acc: 34.38%\n",
      "Acc: 37.5%\n",
      "Acc: 41.41%\n",
      "Acc: 46.09%\n",
      "Acc: 49.22%\n",
      "Acc: 45.31%\n",
      "Acc: 52.34%\n",
      "Acc: 52.34%\n",
      "Acc: 43.75%\n",
      "Acc: 52.34%\n",
      "Acc: 53.12%\n",
      "Acc: 49.22%\n",
      "Acc: 55.47%\n",
      "Acc: 53.91%\n",
      "Acc: 62.5%\n",
      "Acc: 57.81%\n",
      "Acc: 60.94%\n",
      "Acc: 61.72%\n",
      "Acc: 64.84%\n",
      "Acc: 53.12%\n",
      "Acc: 61.72%\n",
      "Acc: 53.91%\n",
      "Acc: 72.66%\n",
      "Acc: 58.59%\n",
      "Acc: 74.22%\n",
      "Acc: 57.81%\n",
      "Acc: 60.16%\n",
      "Acc: 64.06%\n",
      "Acc: 71.88%\n",
      "Acc: 58.59%\n",
      "Acc: 72.66%\n",
      "Acc: 57.81%\n",
      "Acc: 67.19%\n",
      "Acc: 60.94%\n",
      "Acc: 66.41%\n",
      "Acc: 66.41%\n",
      "Acc: 69.53%\n",
      "Acc: 72.66%\n",
      "Acc: 71.09%\n",
      "Acc: 70.31%\n",
      "Acc: 72.66%\n",
      "Acc: 68.75%\n",
      "Acc: 68.75%\n",
      "Acc: 66.41%\n",
      "Acc: 70.31%\n",
      "Acc: 64.06%\n",
      "Acc: 72.66%\n",
      "Acc: 78.12%\n",
      "Acc: 69.53%\n",
      "Acc: 74.22%\n",
      "Acc: 73.44%\n",
      "Acc: 75.0%\n",
      "Acc: 74.22%\n",
      "Acc: 77.34%\n",
      "Acc: 76.56%\n",
      "Acc: 67.97%\n",
      "Acc: 72.66%\n",
      "Acc: 75.78%\n",
      "Acc: 77.34%\n",
      "Acc: 74.22%\n",
      "Acc: 75.0%\n",
      "Acc: 75.0%\n",
      "Acc: 75.78%\n",
      "Acc: 68.75%\n",
      "Acc: 80.47%\n",
      "Acc: 78.91%\n",
      "Acc: 73.44%\n",
      "Acc: 76.56%\n",
      "Acc: 76.56%\n",
      "Acc: 78.12%\n",
      "Acc: 78.12%\n",
      "Acc: 71.88%\n",
      "Acc: 77.34%\n",
      "Acc: 75.78%\n",
      "Acc: 81.25%\n",
      "Acc: 74.22%\n",
      "Acc: 74.22%\n",
      "Acc: 78.91%\n",
      "Acc: 76.56%\n",
      "Acc: 74.22%\n",
      "Acc: 75.78%\n",
      "Acc: 78.12%\n",
      "Acc: 76.56%\n",
      "Acc: 80.47%\n",
      "Acc: 79.69%\n",
      "Acc: 77.34%\n",
      "Acc: 75.0%\n",
      "Acc: 78.91%\n",
      "Acc: 72.66%\n",
      "Acc: 75.78%\n",
      "Acc: 78.91%\n",
      "Acc: 79.69%\n",
      "Acc: 79.69%\n",
      "Acc: 85.94%\n",
      "Acc: 79.69%\n",
      "Acc: 75.0%\n",
      "Acc: 76.56%\n",
      "Acc: 80.47%\n",
      "Acc: 75.0%\n",
      "Acc: 88.28%\n",
      "Acc: 78.91%\n",
      "Acc: 78.12%\n",
      "Acc: 76.56%\n",
      "Acc: 82.81%\n",
      "Acc: 74.22%\n",
      "Acc: 84.38%\n",
      "Acc: 83.59%\n",
      "Acc: 83.59%\n",
      "Acc: 80.47%\n",
      "Acc: 78.12%\n",
      "Acc: 77.34%\n",
      "Acc: 80.47%\n",
      "Acc: 86.72%\n",
      "Acc: 78.91%\n",
      "Acc: 81.25%\n",
      "Acc: 80.47%\n",
      "Acc: 81.25%\n",
      "Acc: 77.34%\n",
      "Acc: 76.56%\n",
      "Acc: 78.12%\n",
      "Acc: 85.16%\n",
      "Acc: 78.91%\n",
      "Acc: 79.69%\n",
      "Acc: 79.69%\n",
      "Acc: 78.91%\n",
      "Acc: 77.34%\n",
      "Acc: 71.09%\n",
      "Acc: 78.91%\n",
      "Acc: 82.81%\n",
      "Acc: 82.81%\n",
      "Acc: 81.25%\n",
      "Acc: 81.25%\n",
      "Acc: 85.16%\n",
      "Acc: 85.94%\n",
      "Acc: 82.03%\n",
      "Acc: 82.81%\n",
      "Acc: 81.25%\n",
      "Acc: 82.03%\n",
      "Acc: 82.03%\n",
      "Acc: 82.81%\n",
      "Acc: 71.88%\n",
      "Acc: 71.88%\n",
      "Acc: 82.03%\n",
      "Acc: 83.59%\n",
      "Acc: 82.81%\n",
      "Acc: 83.59%\n",
      "Acc: 85.16%\n",
      "Acc: 80.47%\n",
      "Acc: 75.0%\n",
      "Acc: 79.69%\n",
      "Acc: 78.91%\n",
      "Acc: 81.25%\n",
      "Acc: 81.25%\n",
      "Acc: 76.56%\n",
      "Acc: 74.22%\n",
      "Acc: 79.69%\n",
      "Acc: 87.5%\n",
      "Acc: 81.25%\n",
      "Acc: 83.59%\n",
      "Acc: 76.56%\n",
      "Acc: 85.16%\n",
      "Acc: 73.44%\n",
      "Acc: 82.03%\n",
      "Acc: 82.81%\n",
      "Acc: 82.03%\n",
      "Acc: 78.91%\n",
      "Acc: 78.12%\n",
      "Acc: 81.25%\n",
      "Acc: 75.78%\n",
      "Acc: 83.59%\n",
      "Acc: 82.81%\n",
      "Acc: 85.16%\n",
      "Acc: 79.69%\n",
      "Acc: 83.59%\n",
      "Acc: 82.03%\n",
      "Acc: 86.72%\n",
      "Acc: 84.38%\n",
      "Acc: 78.12%\n",
      "Acc: 86.72%\n",
      "Acc: 75.78%\n",
      "Acc: 78.12%\n",
      "Acc: 88.28%\n",
      "Acc: 85.94%\n",
      "Acc: 81.25%\n",
      "Acc: 81.25%\n",
      "Acc: 83.59%\n",
      "Acc: 82.81%\n",
      "Acc: 81.25%\n",
      "Acc: 82.03%\n",
      "Acc: 82.03%\n",
      "Acc: 83.59%\n",
      "Acc: 85.94%\n",
      "Acc: 81.25%\n",
      "Acc: 78.91%\n",
      "Acc: 79.69%\n",
      "Acc: 82.03%\n",
      "Acc: 82.03%\n",
      "Acc: 84.38%\n",
      "Acc: 79.69%\n",
      "Acc: 78.12%\n",
      "Acc: 75.78%\n",
      "Acc: 85.16%\n",
      "Acc: 84.38%\n",
      "Acc: 85.16%\n",
      "Acc: 89.06%\n",
      "Acc: 84.38%\n",
      "Acc: 84.38%\n",
      "Acc: 86.72%\n",
      "Acc: 83.59%\n",
      "Acc: 82.81%\n",
      "Acc: 85.94%\n",
      "Acc: 87.5%\n",
      "Acc: 88.28%\n",
      "Acc: 89.06%\n",
      "Acc: 82.81%\n",
      "Acc: 85.94%\n",
      "Acc: 89.84%\n",
      "Acc: 83.59%\n",
      "Acc: 85.16%\n",
      "Acc: 79.69%\n",
      "Acc: 85.94%\n",
      "Acc: 84.38%\n",
      "Acc: 86.72%\n",
      "Acc: 82.03%\n",
      "Acc: 85.94%\n",
      "Acc: 86.72%\n",
      "Acc: 84.38%\n",
      "Acc: 84.38%\n",
      "Acc: 86.72%\n",
      "Acc: 86.72%\n",
      "Acc: 83.59%\n",
      "Acc: 88.28%\n",
      "Acc: 83.59%\n",
      "Acc: 89.06%\n",
      "Acc: 85.16%\n",
      "Acc: 85.16%\n",
      "Acc: 91.41%\n",
      "Acc: 82.81%\n",
      "Acc: 87.5%\n",
      "Acc: 88.28%\n",
      "Acc: 85.16%\n",
      "Acc: 86.72%\n",
      "Acc: 81.25%\n",
      "Acc: 86.72%\n",
      "Acc: 88.28%\n",
      "Acc: 87.5%\n",
      "Acc: 83.59%\n",
      "Acc: 82.03%\n",
      "Acc: 78.91%\n",
      "Acc: 86.72%\n",
      "Acc: 75.0%\n",
      "Acc: 87.5%\n",
      "Acc: 85.94%\n",
      "Acc: 85.94%\n",
      "Acc: 81.25%\n",
      "Acc: 88.28%\n",
      "Acc: 89.06%\n",
      "Acc: 84.38%\n",
      "Acc: 82.03%\n",
      "Acc: 80.47%\n",
      "Acc: 82.03%\n",
      "Acc: 83.59%\n",
      "Acc: 84.38%\n",
      "Acc: 85.94%\n",
      "Acc: 82.03%\n",
      "Acc: 85.16%\n",
      "Acc: 80.47%\n",
      "Acc: 83.59%\n",
      "Acc: 78.91%\n",
      "Acc: 91.41%\n",
      "Acc: 85.94%\n",
      "Acc: 87.5%\n",
      "Acc: 85.16%\n",
      "Acc: 89.06%\n",
      "Acc: 85.94%\n",
      "Acc: 88.28%\n",
      "Acc: 84.38%\n",
      "Acc: 82.03%\n",
      "Acc: 88.28%\n",
      "Acc: 84.38%\n",
      "Acc: 83.59%\n",
      "Acc: 83.59%\n",
      "Acc: 85.16%\n",
      "Acc: 85.16%\n",
      "Acc: 79.69%\n",
      "Acc: 94.53%\n",
      "Acc: 91.41%\n",
      "Acc: 85.94%\n",
      "Acc: 82.03%\n",
      "Acc: 87.5%\n",
      "Acc: 85.94%\n",
      "Acc: 79.69%\n",
      "Acc: 86.72%\n",
      "Acc: 83.59%\n",
      "Acc: 83.59%\n",
      "Acc: 85.16%\n",
      "Acc: 92.97%\n",
      "Acc: 86.72%\n",
      "Acc: 88.28%\n",
      "Acc: 92.19%\n",
      "Acc: 86.72%\n",
      "Acc: 85.94%\n",
      "Acc: 90.62%\n",
      "Acc: 85.94%\n",
      "Acc: 81.25%\n",
      "Acc: 89.06%\n",
      "Acc: 81.25%\n",
      "Acc: 85.94%\n",
      "Acc: 85.94%\n",
      "Acc: 88.28%\n",
      "Acc: 89.84%\n",
      "Acc: 89.06%\n",
      "Acc: 87.5%\n",
      "Acc: 89.06%\n",
      "Acc: 96.09%\n",
      "Acc: 85.16%\n",
      "Acc: 84.38%\n",
      "Acc: 81.25%\n",
      "Acc: 84.38%\n",
      "Acc: 83.59%\n",
      "Acc: 88.28%\n",
      "Acc: 81.25%\n",
      "Acc: 86.72%\n",
      "Acc: 86.72%\n",
      "Acc: 86.72%\n",
      "Acc: 84.38%\n",
      "Acc: 88.28%\n",
      "Acc: 88.28%\n",
      "Acc: 82.81%\n",
      "Acc: 85.94%\n",
      "Acc: 86.72%\n",
      "Acc: 86.72%\n",
      "Acc: 85.16%\n",
      "Acc: 82.03%\n",
      "Acc: 85.16%\n",
      "Acc: 83.59%\n",
      "Acc: 88.28%\n",
      "Acc: 89.06%\n",
      "Acc: 81.25%\n",
      "Acc: 88.28%\n",
      "Acc: 89.84%\n",
      "Acc: 86.72%\n",
      "Acc: 88.28%\n",
      "Acc: 87.5%\n",
      "Acc: 88.28%\n",
      "Acc: 89.06%\n",
      "Acc: 89.84%\n",
      "Acc: 85.94%\n",
      "Acc: 89.84%\n",
      "Acc: 86.72%\n",
      "Acc: 86.72%\n",
      "Acc: 89.06%\n",
      "Acc: 72.66%\n",
      "Acc: 90.62%\n",
      "Acc: 85.16%\n",
      "Acc: 88.28%\n",
      "Acc: 80.47%\n",
      "Acc: 82.81%\n",
      "Acc: 86.72%\n",
      "Acc: 85.94%\n",
      "Acc: 84.38%\n",
      "Acc: 91.41%\n",
      "Acc: 90.62%\n",
      "Acc: 85.94%\n",
      "Acc: 85.94%\n",
      "Acc: 89.84%\n",
      "Acc: 82.03%\n",
      "Acc: 85.94%\n",
      "Acc: 85.94%\n",
      "Acc: 82.81%\n",
      "Acc: 84.38%\n",
      "Acc: 89.06%\n",
      "Acc: 88.28%\n",
      "Acc: 83.59%\n",
      "Acc: 86.72%\n",
      "Acc: 85.94%\n",
      "Acc: 87.5%\n",
      "Acc: 84.38%\n",
      "Acc: 86.72%\n",
      "Acc: 85.94%\n",
      "Acc: 85.94%\n",
      "Acc: 85.16%\n",
      "Acc: 85.94%\n",
      "Acc: 82.03%\n",
      "Acc: 84.38%\n",
      "Acc: 88.28%\n",
      "Acc: 85.16%\n",
      "Acc: 79.69%\n",
      "Acc: 88.28%\n",
      "Acc: 88.28%\n",
      "Acc: 88.28%\n",
      "Acc: 85.16%\n",
      "Acc: 82.81%\n",
      "Acc: 86.72%\n",
      "Acc: 88.28%\n",
      "Acc: 89.84%\n",
      "Acc: 89.84%\n",
      "Acc: 82.81%\n",
      "Acc: 89.06%\n",
      "Acc: 84.38%\n",
      "Acc: 86.72%\n",
      "Acc: 89.06%\n",
      "Acc: 83.59%\n",
      "Acc: 88.28%\n",
      "Acc: 92.19%\n",
      "Acc: 83.59%\n",
      "Acc: 86.72%\n",
      "Acc: 87.5%\n",
      "Acc: 87.5%\n",
      "Acc: 81.25%\n",
      "Acc: 86.72%\n",
      "Acc: 85.16%\n",
      "Acc: 86.72%\n",
      "Acc: 82.81%\n",
      "Acc: 89.84%\n",
      "Acc: 92.19%\n",
      "Acc: 89.84%\n",
      "Acc: 89.84%\n",
      "Acc: 91.41%\n",
      "Acc: 87.5%\n",
      "Acc: 85.16%\n",
      "Acc: 85.94%\n",
      "Acc: 85.94%\n",
      "Acc: 86.72%\n",
      "Acc: 85.94%\n",
      "Acc: 87.5%\n",
      "Acc: 91.41%\n",
      "Acc: 85.94%\n",
      "Acc: 89.06%\n",
      "Acc: 89.06%\n",
      "Acc: 88.28%\n",
      "Acc: 85.94%\n",
      "Acc: 82.81%\n",
      "Acc: 87.5%\n",
      "Acc: 87.5%\n",
      "Acc: 89.06%\n",
      "Acc: 85.16%\n",
      "Acc: 87.5%\n",
      "Acc: 84.38%\n",
      "Acc: 92.19%\n",
      "Acc: 86.72%\n",
      "Acc: 86.72%\n",
      "Acc: 90.62%\n",
      "Acc: 89.84%\n",
      "Acc: 83.59%\n",
      "Acc: 85.16%\n",
      "Acc: 85.94%\n",
      "Acc: 87.5%\n",
      "Acc: 90.62%\n",
      "Acc: 92.97%\n",
      "Acc: 89.06%\n",
      "Acc: 91.41%\n",
      "Acc: 86.72%\n",
      "Acc: 84.38%\n",
      "Acc: 85.94%\n",
      "Acc: 83.59%\n",
      "Acc: 86.72%\n",
      "Acc: 88.28%\n",
      "Acc: 82.03%\n",
      "Acc: 91.41%\n",
      "Acc: 91.41%\n",
      "Acc: 86.72%\n",
      "Acc: 90.62%\n",
      "Acc: 87.5%\n",
      "Acc: 89.84%\n",
      "Acc: 89.84%\n",
      "Acc: 85.94%\n",
      "Acc: 90.62%\n",
      "Acc: 89.06%\n",
      "Acc: 85.94%\n",
      "Acc: 87.5%\n",
      "Acc: 92.97%\n",
      "Acc: 87.5%\n",
      "Acc: 88.28%\n",
      "Acc: 85.16%\n",
      "Acc: 89.84%\n",
      "Acc: 89.06%\n",
      "Acc: 86.72%\n",
      "Acc: 89.84%\n",
      "Acc: 93.75%\n"
     ]
    }
   ],
   "source": [
    "n = Network([784,128,10])\n",
    "\n",
    "n.SGD(data,labels,500,128,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a98d113f-ac70-4d82-bea1-7e8341e1329b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAANu0lEQVR4nO3df6zd9V3H8deL7raMsi2tSC1QR2FsC45I9abMQRTDRgrRFZwiNS5V2e5ihoNsGRJmHDEaCcgmLmOmBUJtBmTZQGokk9pg2Nyo3EKlLR2UYZF2pbVjGQVcuW3f/nG/zEu553Nuv9/z6/b9fCQ355zv+3y/33dO+ur3nO/nnO/HESEAR79j+t0AgN4g7EAShB1IgrADSRB2IIm39HJnMz0rjtXsXu4SSOUnekWvxX5PVmsUdttLJN0iaYak2yLihtLzj9VsneMLmuwSQMH6WNeyVvttvO0Zkr4s6SJJZ0paZvvMutsD0F1NPrMvlvRMRDwbEa9JukfS0s60BaDTmoT9ZEnPT3i8o1r2BrZHbI/aHh3T/ga7A9BE18/GR8SKiBiOiOEhzer27gC00CTsOyUtmPD4lGoZgAHUJOyPSjrD9kLbMyVdLmlNZ9oC0Gm1h94i4oDtKyX9i8aH3u6IiC0d6wxARzUaZ4+IByQ90KFeAHQRX5cFkiDsQBKEHUiCsANJEHYgCcIOJNHT37Mjnxm/8J6WtfsfvKu47vLtHyzWf3juj2r1lBVHdiAJwg4kQdiBJAg7kARhB5Ig7EASDL2hkZeWvb9Y/+VPP96yNhYHi+se0qRXREZNHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2dHIqz9XPl7cOP9btbf9xDffW6wv0HdqbzsjjuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7Ghk5aduqb3u118+qVhfuHpHsX6g9p5zahR229sl7ZN0UNKBiBjuRFMAOq8TR/Zfj4i9HdgOgC7iMzuQRNOwh6QHbW+wPTLZE2yP2B61PTqm/Q13B6Cupm/jz4uInbZPlLTW9vci4uGJT4iIFZJWSNLbPTca7g9ATY2O7BGxs7rdI+k+SYs70RSAzqsddtuzbb/t9fuSLpS0uVONAeisJm/j50m6z/br27krIr7Zka4wMH7w2Q8U64tmbijWS9eG/4t//u3iuqdvf6RYx5GpHfaIeFbSL3awFwBdxNAbkARhB5Ig7EAShB1IgrADSfAT1+S+f9OvFOtbf+9LxfqQZxTr/33gf1vW5m5iSuZe4sgOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzn4U8NDMlrXnPle+4O+//e6NxfpYtN62VB5Hl6SP3HxNy9q8O5lyuZc4sgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoyzHwWOeffClrUNH/vbNmuXx9Hb+Y3/+ONi/ef/jrH0QcGRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJx9GnjLwncW62/9+x/W3vbWsXL9YzddXayfdv/2Yv3AkbWDLmp7ZLd9h+09tjdPWDbX9lrb26rbOd1tE0BTU3kbf6ekJYctu1bSuog4Q9K66jGAAdY27BHxsKQXD1u8VNKq6v4qSZd0ti0AnVb3M/u8iNhV3X9B0rxWT7Q9ImlEko7VcTV3B6CpxmfjIyIkRaG+IiKGI2J4SLOa7g5ATXXDvtv2fEmqbvd0riUA3VA37GskLa/uL5d0f2faAdAtbT+z275b0vmSTrC9Q9LnJd0g6Wu2r5D0nKTLutlkdifds7dYv+Xkh2pv+6YfHD7Q8kYn3lr+PTrj6NNH27BHxLIWpQs63AuALuLrskAShB1IgrADSRB2IAnCDiTBT1yngZUL/r1YH4sZtbe9+tS15SfsLJeHXN73WBw8wo7+31m3X1msL7z3x8X6oY1P1t730YgjO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwTh7Dxwze3ax/tRfv69YH4sNber1x7K7rUlvj/3RLcX6io+8u1i/9Z8uallbeO13a/U0nXFkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkGGfvgZcvLI+jb/6tL7XZQv3fqzf1oc2XF+u7t5zYtX3POOXVYv3xc28r1n98UevpxtavPqu47sEtTxXr0xFHdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2HvjAn6/v6vb3HnqtZe2a5z9cXvfPTi3W37Hxv4r143/0bLHexKuXnlN+wrnl8nUnbGpZu+Bd5xXXfeuW8rano7ZHdtt32N5je/OEZdfb3ml7Y/V3cXfbBNDUVN7G3ylpySTLvxgRZ1d/D3S2LQCd1jbsEfGwpBd70AuALmpygu5K209Ub/PntHqS7RHbo7ZHx7S/we4ANFE37F+RdLqksyXtknRzqydGxIqIGI6I4SHNqrk7AE3VCntE7I6IgxFxSNJKSYs72xaATqsVdtvzJzy8VNLmVs8FMBjajrPbvlvS+ZJOsL1D0uclnW/7bEkhabukT3Svxelv0XHPdXX7F9z12Za109pcH31Gm3Ovg3tFehyptmGPiGWTLL69C70A6CK+LgskQdiBJAg7kARhB5Ig7EAS/MS1B4ZcHsAacvlS0e3q8x+ZngNkh35tUbG+84Pl9du9Lnfvm9eydtzO8mWqo7zraYkjO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwTh7D4xFeTx4LJqNk5/w6daXe/7Jtvc02nZTT1/Xetrkvxz+x+K6H569u1j/7v6Zxfpdvz/ZdVLHxWi+SzBwZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBzRu1/uvt1z4xxf0LP9DYqnbxsu1rcsubVYb/e77abj9E10s7evv3xSsb76479ZrB/zrcdr73u6Wh/r9FK86MlqHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAl+z94D771qa7F+1l99qlj/3u98uZPt9NTeQ6+1rC1ZeU1x3YWrdxTrx2zPN47eRNsju+0Fth+y/aTtLbavqpbPtb3W9rbqdk732wVQ11Texh+Q9JmIOFPS+yV90vaZkq6VtC4izpC0rnoMYEC1DXtE7IqIx6r7+yRtlXSypKWSVlVPWyXpki71CKADjugzu+1TJS2StF7SvIjYVZVekDTpxFq2RySNSNKxan09MgDdNeWz8baPl/QNSVdHxEsTazH+a5pJf1ETESsiYjgihoc0q1GzAOqbUthtD2k86F+NiHurxbttz6/q8yXt6U6LADqh7dt425Z0u6StEfGFCaU1kpZLuqG6vb8rHR4FDr3ySrH+rqsfKdaHt/9Jsf6HH3+gZW3kHU8X121q+KZyb0P7Wv+EesHt3ymue6BWR2hlKp/Zz5X0UUmbbG+sll2n8ZB/zfYVkp6TdFlXOgTQEW3DHhHfljTpj+El5bsSBTBN8XVZIAnCDiRB2IEkCDuQBGEHkuBS0sBRhEtJAyDsQBaEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEk2obd9gLbD9l+0vYW21dVy6+3vdP2xurv4u63C6CuqczPfkDSZyLiMdtvk7TB9tqq9sWI+JvutQegU6YyP/suSbuq+/tsb5V0crcbA9BZR/SZ3fapkhZJWl8tutL2E7bvsD2nxTojtkdtj45pf7NuAdQ25bDbPl7SNyRdHREvSfqKpNMlna3xI//Nk60XESsiYjgihoc0q3nHAGqZUthtD2k86F+NiHslKSJ2R8TBiDgkaaWkxd1rE0BTUzkbb0m3S9oaEV+YsHz+hKddKmlz59sD0ClTORt/rqSPStpke2O17DpJy2yfLSkkbZf0iS70B6BDpnI2/tuSJpvv+YHOtwOgW/gGHZAEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAlHRO92Zv+PpOcmLDpB0t6eNXBkBrW3Qe1Lore6OtnbOyPiZycr9DTsb9q5PRoRw31roGBQexvUviR6q6tXvfE2HkiCsANJ9DvsK/q8/5JB7W1Q+5Lora6e9NbXz+wAeqffR3YAPULYgST6EnbbS2w/ZfsZ29f2o4dWbG+3vamahnq0z73cYXuP7c0Tls21vdb2tup20jn2+tTbQEzjXZhmvK+vXb+nP+/5Z3bbMyQ9LelDknZIelTSsoh4sqeNtGB7u6ThiOj7FzBs/6qklyX9Q0S8r1p2o6QXI+KG6j/KORHxpwPS2/WSXu73NN7VbEXzJ04zLukSSX+gPr52hb4uUw9et34c2RdLeiYino2I1yTdI2lpH/oYeBHxsKQXD1u8VNKq6v4qjf9j6bkWvQ2EiNgVEY9V9/dJen2a8b6+doW+eqIfYT9Z0vMTHu/QYM33HpIetL3B9ki/m5nEvIjYVd1/QdK8fjYzibbTePfSYdOMD8xrV2f686Y4Qfdm50XEL0m6SNInq7erAynGP4MN0tjplKbx7pVJphn/qX6+dnWnP2+qH2HfKWnBhMenVMsGQkTsrG73SLpPgzcV9e7XZ9Ctbvf0uZ+fGqRpvCebZlwD8Nr1c/rzfoT9UUln2F5oe6akyyWt6UMfb2J7dnXiRLZnS7pQgzcV9RpJy6v7yyXd38de3mBQpvFuNc24+vza9X3684jo+Z+kizV+Rv77kj7Xjx5a9HWapP+s/rb0uzdJd2v8bd2Yxs9tXCHpZyStk7RN0r9KmjtAva2WtEnSExoP1vw+9Xaext+iPyFpY/V3cb9fu0JfPXnd+LoskAQn6IAkCDuQBGEHkiDsQBKEHUiCsANJEHYgif8DCAYepsjcktkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xb/tql77jt13z7fd6cqtftw62nm0000gn/T/ipykernel_7703/2810796055.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp( -x ))\n"
     ]
    }
   ],
   "source": [
    "import random as r\n",
    "x = r.randrange(0,60000)\n",
    "image = np.asarray(data[x].squeeze())\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "print(n.feedforward(data[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed8a873-c268-4e40-a9b6-0f4a2f30f61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725c6fa-fd00-49d4-a4f8-79cf06596571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
